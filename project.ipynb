{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected and saved to zomato_restaurants_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "# List of restaurant links based on localities\n",
    "localities = [\n",
    "    \"https://www.zomato.com/hyderabad/begumpet-restaurants\",\n",
    "    # Add more locality URLs as needed...\n",
    "]\n",
    "\n",
    "# Initialize lists to store data\n",
    "all_urls = []\n",
    "all_rest_name = []\n",
    "all_ratings = []\n",
    "all_price = []\n",
    "all_cuisine = []\n",
    "all_images = []  # For image URLs\n",
    "all_opening_hours = []\n",
    "all_locations = []\n",
    "all_signature_dishes = []  # Popular dishes\n",
    "all_special_features = []  # What people say about the restaurant\n",
    "all_safety_measures = []  # Safety and hygiene measures\n",
    "all_address = []  # Address of the restaurant\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "for link in localities:\n",
    "    driver.get(link)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Scroll and load all contents for the current locality\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for new content to load\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # Break if no more content is loaded\n",
    "        last_height = new_height\n",
    "\n",
    "    # Parse the page source using BeautifulSoup after loading all content\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    divs = soup.findAll('div', class_='jumbo-tracker')\n",
    "\n",
    "    for parent in divs:\n",
    "        name_tag = parent.find(\"h4\")\n",
    "        if name_tag is not None:\n",
    "            rest_name = name_tag.text.strip()\n",
    "            link_tag = parent.find(\"a\")\n",
    "            restaurant_link = urljoin(\"https://www.zomato.com\", link_tag.get('href'))\n",
    "\n",
    "            try:\n",
    "                driver.get(restaurant_link)\n",
    "                time.sleep(3)  # Allow time for the page to load\n",
    "                \n",
    "                inner_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                rating_tag = inner_soup.find('div', class_='sc-1q7bklc-1 cILgox')\n",
    "                rating_value = rating_tag.text.strip() if rating_tag else 'Not available'\n",
    "\n",
    "                # Extract price using the specific class you provided\n",
    "                price_tag = inner_soup.find('p', class_='sc-1hez2tp-0 sc-adtsK iuDANL')\n",
    "                price_value = price_tag.text.strip() if price_tag else 'Not available'\n",
    "\n",
    "                cuisine_tag = inner_soup.find('div', class_='sc-fgfRvd gBMRZZ')\n",
    "                cuisine_value = cuisine_tag.text.strip() if cuisine_tag else 'Not available'\n",
    "\n",
    "                open_timing_tag = inner_soup.find('span', class_='sc-kasBVs dfwCXs')\n",
    "                open_timing_value = open_timing_tag.text.strip() if open_timing_tag else 'Not available'\n",
    "\n",
    "                location_tag = inner_soup.find('a', class_='sc-clNaTc vNCcy')\n",
    "                location_value = location_tag.text.strip() if location_tag else 'Not available'\n",
    "\n",
    "                popular_dishes_tag = inner_soup.find('h3', string='Popular Dishes')  # Updated to use string instead of text\n",
    "                signature_dishes_text_value = popular_dishes_tag.find_next('p').text.strip() if popular_dishes_tag else 'Not available'\n",
    "\n",
    "                people_say_tag = inner_soup.find('h3', string='People Say This Place Is Known For')  # Updated to use string instead of text\n",
    "                special_features_text_value = people_say_tag.find_next('p').text.strip() if people_say_tag else 'Not available'\n",
    "\n",
    "                image_tag = inner_soup.find('img', class_='sc-s1isp7-5 fyZwWD')\n",
    "                image_url_value = image_tag.get(\"src\") if image_tag else None\n",
    "\n",
    "                safety_measures_section_1 = inner_soup.find('section', class_='sc-bgxRrC fHqOaY')\n",
    "                safety_measures_value_list_items_1 = safety_measures_section_1.find_all('p') if safety_measures_section_1 else []\n",
    "\n",
    "                safety_measures_section_2_items= inner_soup.find_all('p', class_='sc-1hez2tp-0 fvARMW')  \n",
    "                \n",
    "                all_safety_measures_items = [item.text.strip() for item in safety_measures_value_list_items_1]\n",
    "                all_safety_measures_items += [item.text.strip() for item in safety_measures_section_2_items]\n",
    "\n",
    "                safety_measures_value_final = \", \".join(all_safety_measures_items) if all_safety_measures_items else 'Not available'\n",
    "\n",
    "                address_section = inner_soup.find('p', class_='sc-1hez2tp-0 clKRrC')\n",
    "                address_value = address_section.text.strip() if address_section else 'Not available'\n",
    "\n",
    "                all_urls.append(restaurant_link)\n",
    "                all_rest_name.append(rest_name)\n",
    "                all_ratings.append(rating_value)\n",
    "                all_price.append(price_value)\n",
    "                all_cuisine.append(cuisine_value)\n",
    "                all_images.append(image_url_value)\n",
    "                all_opening_hours.append(open_timing_value)\n",
    "                all_locations.append(location_value)\n",
    "                all_signature_dishes.append(signature_dishes_text_value)\n",
    "                all_special_features.append(special_features_text_value)\n",
    "                all_safety_measures.append(safety_measures_value_final)\n",
    "                all_address.append(address_value)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {restaurant_link}: {e}\")\n",
    "        \n",
    "            finally:\n",
    "                driver.back()  # Navigate back to the locality page\n",
    "                time.sleep(3)  # Wait for the page to load again\n",
    "\n",
    "# Close the WebDriver after scraping\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame to store the collected data\n",
    "df = pd.DataFrame({\n",
    "    'links': all_urls,\n",
    "    'names': all_rest_name,\n",
    "    'ratings': all_ratings,\n",
    "    'price for two': all_price,\n",
    "    'cuisine': all_cuisine,\n",
    "    'images': all_images,\n",
    "    'opening & closing time': all_opening_hours,\n",
    "    'location': all_locations,\n",
    "    'signature dishes': all_signature_dishes,\n",
    "    'special features': all_special_features,\n",
    "    'safety measures': all_safety_measures,\n",
    "    'address': all_address\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('zomato_restaurants_data.csv', index=False)\n",
    "print(\"Data collected and saved to zomato_restaurants_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Links -\n",
    "2. Restaurant Names-\n",
    "3. Location & Accessibility (Address)-\n",
    "4. Cuisine Type-\n",
    "5. Price Range (Per 2)-\n",
    "6. Reviews & Ratings (Out of 5)-\n",
    "7. Opening Hours & Reservation Policies (Timings)-\n",
    "8. Special Features-\n",
    "9. Signature Dishes-\n",
    "10. Service Quality\n",
    "11. Dietary Preferences\n",
    "12. Hygiene & Safety\n",
    "13. Crowd & Wait Time\n",
    "14. Family-Friendliness\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "# List of restaurant links based on localities\n",
    "localities = [\n",
    "    \"https://www.zomato.com/hyderabad/jubilee-hills-restaurants\",\n",
    "    \"https://www.zomato.com/hyderabad/gachibowli-restaurants\",\n",
    "    # Add more locality URLs as needed...\n",
    "]\n",
    "\n",
    "# Initialize lists to store data\n",
    "all_data = []\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "for link in localities:\n",
    "    driver.get(link)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Scroll and load all contents for the current locality\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for new content to load\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # Break if no more content is loaded\n",
    "        last_height = new_height\n",
    "\n",
    "    # Parse the page source using BeautifulSoup after loading all content\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    divs = soup.findAll('div', class_='jumbo-tracker')  # Adjust class as needed based on Zomato's HTML structure\n",
    "\n",
    "    for parent in divs:\n",
    "        # Extract restaurant name and link\n",
    "        name_tag = parent.find(\"h4\")\n",
    "        if name_tag is not None:\n",
    "            rest_name = name_tag.text.strip()\n",
    "            link_tag = parent.find(\"a\")\n",
    "            restaurant_link = urljoin(\"https://www.zomato.com\", link_tag.get('href'))\n",
    "\n",
    "            try:\n",
    "                # Click on the restaurant link to open its page\n",
    "                driver.get(restaurant_link)\n",
    "                time.sleep(2)  # Allow time for the page to load\n",
    "\n",
    "                # Extract data from the individual restaurant page\n",
    "                inner_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                rating_tag = inner_soup.find('div', class_='sc-1q7bklc-1 cILgox')\n",
    "                rating_value = rating_tag.text.strip() if rating_tag else 'Not available'\n",
    "                \n",
    "                price_tag = inner_soup.find('p', class_='sc-1hez2tp-0 sc-adtsK iuDANL')\n",
    "                price_value = price_tag.text.strip() if price_tag else 'Not available'\n",
    "\n",
    "                cuisine_tag = inner_soup.find('div', class_='sc-fgfRvd gBMRZZ')\n",
    "                cuisine_value = cuisine_tag.text.strip() if cuisine_tag else 'Not available'\n",
    "\n",
    "                open_timing_tag = inner_soup.find('span', class_='sc-kasBVs dfwCXs')\n",
    "                open_timing_value = open_timing_tag.text.strip() if open_timing_tag else 'Not available'\n",
    "\n",
    "                location_tag = inner_soup.find('a', class_='sc-clNaTc vNCcy')\n",
    "                location_value = location_tag.text.strip() if location_tag else 'Not available'\n",
    "\n",
    "                popular_dishes_tag = inner_soup.find('h3', text='Popular Dishes')\n",
    "                signature_dishes_text_value = popular_dishes_tag.find_next('p').text.strip() if popular_dishes_tag else 'Not available'\n",
    "\n",
    "                people_say_tag = inner_soup.find('h3', text='People Say This Place Is Known For')\n",
    "                special_features_text_value = people_say_tag.find_next('p').text.strip() if people_say_tag else 'Not available'\n",
    "\n",
    "                image_tag = inner_soup.find('img', class_='sc-s1isp7-5 fyZwWD')\n",
    "                image_url_value = image_tag.get(\"src\") if image_tag else None\n",
    "\n",
    "                safety_measures_section_1 = inner_soup.find('section', class_='sc-bgxRrC fHqOaY')\n",
    "                safety_measures_value_list_items_1 = safety_measures_section_1.find_all('p') if safety_measures_section_1 else []\n",
    "\n",
    "                safety_measures_section_2_items= inner_soup.find_all('p', class_='sc-1hez2tp-0 fvARMW')  \n",
    "                \n",
    "                all_safety_measures_items = [item.text.strip() for item in safety_measures_value_list_items_1]\n",
    "                all_safety_measures_items += [item.text.strip() for item in safety_measures_section_2_items]\n",
    "\n",
    "                safety_measures_value_final = \", \".join(all_safety_measures_items) if all_safety_measures_items else 'Not available'\n",
    "\n",
    "                address_section = inner_soup.find('p', class_='sc-1hez2tp-0 clKRrC')\n",
    "                address_value = address_section.text.strip() if address_section else 'Not available'\n",
    "\n",
    "                # Append extracted data to the list\n",
    "                all_data.append({\n",
    "                    \"Restaurant Name\": rest_name,\n",
    "                    \"Link\": restaurant_link,\n",
    "                    \"Rating\": rating_value,\n",
    "                    \"Price\": price_value,\n",
    "                    \"Cuisine\": cuisine_value,\n",
    "                    \"Image URL\": image_url_value,\n",
    "                    \"Opening Hours\": open_timing_value,\n",
    "                    \"Location\": location_value,\n",
    "                    \"Signature Dishes\": signature_dishes_text_value,\n",
    "                    \"Special Features\": special_features_text_value,\n",
    "                    \"Safety Measures\": safety_measures_value_final,\n",
    "                    \"Address\": address_value\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {restaurant_link}: {e}\")\n",
    "            finally:\n",
    "                driver.back()  # Navigate back to the locality page\n",
    "\n",
    "# Close the WebDriver after scraping\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame to store the collected data\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('zomato_restaurants_data.csv', index=False)\n",
    "print(\"Data scraped and saved to zomato_restaurants_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
